{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTCLoss(size_average=False, length_average=False)\n",
    "    size_average (bool): normalize the loss by the batch size (default: False)\n",
    "    通过批量大小规范化损失（默认值：False）\n",
    "    length_average (bool): normalize the loss by the total number of frames in the batch. If True, supersedes size_average (default: False)\n",
    "\n",
    "forward(acts, labels, act_lens, label_lens)\n",
    "    acts: Tensor of (seqLength x batch x outputDim) containing output activations from network (before softmax)\n",
    "    labels: 1 dimensional Tensor containing all the targets of the batch in one large sequence\n",
    "    act_lens: Tensor of size (batch) containing size of each output sequence from the network\n",
    "    label_lens: Tensor of (batch) containing label length of each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from warpctc_pytorch import CTCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.4629\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ctc_loss = CTCLoss().cuda()\n",
    "\n",
    "# expected shape of seqLength x batchSize x alphabet_size\n",
    "probs = torch.FloatTensor([[[0.1, 0.6, 0.1, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]]]).transpose(0, 1).contiguous()\n",
    "\n",
    "labels = Variable(torch.IntTensor([1, 2]))\n",
    "\n",
    "label_sizes = Variable(torch.IntTensor([2]))\n",
    "\n",
    "probs_sizes = Variable(torch.IntTensor([2]))\n",
    "\n",
    "probs = Variable(probs, requires_grad=True) # tells autograd to compute gradients for probs\n",
    "\n",
    "cost = ctc_loss(probs, labels, probs_sizes, label_sizes)\n",
    "\n",
    "cost.backward()\n",
    "\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " (0 ,.,.) = \n",
       "   0.1000  0.6000  0.1000  0.1000  0.1000\n",
       " \n",
       " (1 ,.,.) = \n",
       "   0.1000  0.1000  0.6000  0.1000  0.1000\n",
       " [torch.FloatTensor of size 2x1x5], Variable containing:\n",
       "  1\n",
       "  2\n",
       " [torch.IntTensor of size 2], Variable containing:\n",
       "  2\n",
       " [torch.IntTensor of size 1], Variable containing:\n",
       "  2\n",
       " [torch.IntTensor of size 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs,labels,label_sizes,probs_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0.1000  0.6000  0.1000  0.1000  0.1000\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.1000  0.1000  0.6000  0.1000  0.1000\n",
       "[torch.FloatTensor of size 2x1x5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([[[0.1, 0.6, 0.1, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]]]).transpose(0, 1).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTCLoss(size_average=False, length_average=False)\n",
    "    # size_average (bool): normalize the loss by the batch size (default: False)\n",
    "    # length_average (bool): normalize the loss by the total number of frames in the batch. If True, supersedes size_average (default: False)\n",
    "\n",
    "forward(acts, labels, act_lens, label_lens)\n",
    "    # acts: Tensor of (seqLength x batch x outputDim) containing output activations from network (before softmax)\n",
    "    # labels: 1 dimensional Tensor containing all the targets of the batch in one large sequence\n",
    "    # act_lens: Tensor of size (batch) containing size of each output sequence from the network\n",
    "    # label_lens: Tensor of (batch) containing label length of each example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTCLoss（size_average = False，length_average = False）\n",
    "\n",
    "＃size_average（bool）：通过批量大小规范化损失（默认值：False）\n",
    "\n",
    "＃length_average（bool）：将损失按批次中的总帧数标准化。 如果为True，则取代size_average（默认值：False）\n",
    "转发（行为，标签，act_lens，label_lens）\n",
    "\n",
    "＃acts：包含网络输出激活（softmax之前）的（seqLength x batch x outputDim）张量\n",
    "\n",
    "＃标签：1维张量，其中包含批次的所有目标\n",
    "\n",
    "＃act_lens：包含网络中每个输出序列大小的大小（批量）的张量\n",
    "\n",
    "＃label_lens：包含每个示例的标签长度的（批量）张量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
